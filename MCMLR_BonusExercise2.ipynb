{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4p0ZbepOFaWK2GOZt41J9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/MCMLR/blob/main/MCMLR_BonusExercise2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second Bonus Exercise: Information Extraction\n",
        "\n",
        "This second bonus exercise will focus on methods for information extraction. We will first load an article from a website and then extract information from the article step by step. \n",
        "\n",
        "Each task you have to complete for this bonus exercise is marked with a \"TASK\" heading. \n",
        "\n",
        "\n",
        "*Step-wise instructions for this exercise:*\n",
        "\n",
        "\n",
        "1.   Locate and complete all subtasks of this bonus exercise marked with \"TASK\"\n",
        "2.   Complete each \"TASK\" directly in this notebook and finally submit this notebook for the second bonus exercise\n",
        "\n",
        "---\n",
        "To make it easier to spot, each TASK is separated by lines just like this sentence.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FKjMiwl5JTb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Getting and parsing a news article\n",
        "\n",
        "In order to perform information extraction, we require and need to clean some news article from the web. To this end, a very convenient Python library \"newspaper3k\" has been provided. "
      ],
      "metadata": {
        "id": "mUMLqSQaOl5G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkvMBWBiJHoE"
      },
      "outputs": [],
      "source": [
        "!pip3 install newspaper3k"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import newspaper\n",
        "from newspaper import Article\n",
        "\n",
        "url = 'https://www.aljazeera.com/sports/2022/12/13/messi-leads-argentina-to-world-cup-final-in-3-0-win-over-croatiahttps://www.aljazeera.com/sports/2022/12/13/messi-leads-argentina-to-world-cup-final-in-3-0-win-over-croatia'\n",
        "\n",
        "article = Article(url)\n",
        "article.download()\n",
        "article.parse()\n",
        "\n",
        "#This line displays the authors of the article\n",
        "print(\"Authors: \", article.authors, \"\\n\")\n",
        "\n",
        "#This line displays the entire text of the article\n",
        "print(\"Text of article: \\n\", article.text)"
      ],
      "metadata": {
        "id": "9PldVivpNPEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the article is very long, let's only use the first 10 sentences. To do this, we first need to segment the simple string of ```article.text``` by sentences and then compile these 10 sentences back to a single string (text).  \n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "W5QxfP4mT6cQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "short_article = \" \".join(article.text.split(\"\\n\\n\")[:5])\n",
        "print(\"First 5 sentences of the article :\", short_article)"
      ],
      "metadata": {
        "id": "rIa9BjGxSEQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Getting started with NER\n",
        "\n",
        "The first step of traditional information extraction is to detect all named entities in a document. "
      ],
      "metadata": {
        "id": "BQFFsPQwLHCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_lg\n",
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "hiNzT0oiJTER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp_sm = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "bLZ3VYVgKwL8"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp_sm(short_article)\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "tbtLB1txLxr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# TASK #1: \n",
        "Can you spot any mistakes in the NER output above? Which ones? \n",
        "\n",
        "\n",
        "*Tip: to obtain information on the NER tags, use spacy's explain function, e.g. for \"GPE\" use ``` # spacy.explain(\"GPE\") ```.*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q0U9JZEocXxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To facilitate this task, here the code to only output named entities from the text\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n",
        "\n",
        "#The above code can also be written in one line\n",
        "ents = [(e.text, e.label_) for e in doc.ents]"
      ],
      "metadata": {
        "id": "XbZ_upZ2dYpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your answer for TASK #1:\n",
        "\n",
        "*Type your answer here*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QEDq1OUDdDht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# TASK #2: \n",
        "\n",
        "Does the NER result change if you use the large or TRF (English Transformer \n",
        "pipeline based on RoBERTa)?\n",
        "\n",
        "*Reference: If you would like to know details about transformers, take a look at [this](https://jalammar.github.io/illustrated-transformer/) or [this juxtaposition of different types of utilizations of transfomers](https://youtu.be/xI0HHN5XKDo) .*\n"
      ],
      "metadata": {
        "id": "f2iMN94EUrLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Try NER with the large or TRF model of spaCy - how can this be done? "
      ],
      "metadata": {
        "id": "5Y_kaXEyeZBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your answer for TASK #2: \n",
        "\n",
        "*How does the result change with a different model? Which one did you choose?*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nNIXvn7cekPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependency parsing\n",
        "\n",
        "Named entity recognition is very useful to identify named entities in isolation or as they relate to specific entities, e.g. geographical units, persons, etc. \n",
        "\n",
        "With dependency parsing first types of relations between entities can be identified. While this is not information extraction, it can provide access to first relations between entities (not just named entites) in a sentence and can be and has been frequently utilized as a predecessor of information extraction."
      ],
      "metadata": {
        "id": "P0Vog-_DfoCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp_sm(short_article)\n",
        "\n",
        "#Let's get the second sentence of our article (to get the first the index number would have to be [0] instead of [1])\n",
        "sent = list(doc.sents)[1]\n",
        "print(\"This first sentence is:\")\n",
        "print(sent)\n",
        "\n",
        "#And see its dependency parsing output visualized with spaCy\n",
        "displacy.render(sent, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "id": "14U0WyQn1iqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Entity linking \n",
        "\n",
        "We will now combine named entity recognition and dependency parsing to perform entity linking. In other words, first the named entities of a sentence need to be uncovered and then we need to find out how they depend on each other and with which verb. \n",
        "\n",
        "The following code cell provides all noun phrases in the sentence. "
      ],
      "metadata": {
        "id": "cON-CPGzRjax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input sentence: \", sent, \"\\n\")\n",
        "\n",
        "print(\"All the noun phrases detected in the sentence and their depdendency relations:\")\n",
        "for chunk in sent.noun_chunks:\n",
        "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
        "            chunk.root.head.text)"
      ],
      "metadata": {
        "id": "kReMlhShNTl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## TASK #3: \n",
        "\n",
        "Utilize the named entities extracted above and the dependency parsing output to extract entities and their links (verbs usually) provided by dependency parsing. \n",
        "\n",
        "Pay attention to use the **most** correct version of the spaCy model, e.g. the small model might produce errors in detecting all words belonging to a named entity (\"Messi\" instead of \"Lionel Messi\" in our example sentence). \n",
        "\n",
        "##Your answer for TASK #3: \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pH6OMt1i5_uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Provide your code to combine NER and dependency parsing here"
      ],
      "metadata": {
        "id": "AsQa7gqiVCIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*Please describe here how easy this task was and how well it worked*\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "W33N_MoU6aoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question-Answering\n",
        "\n",
        "Question-Answering can be utilized effectively for information extraction, especially if there is no training data available for a language, e.g. zero-shot transfer. \n",
        "\n",
        "In the code cell below, you find an example of how [DistilBERT base cased distilled SQuAD](https://huggingface.co/distilbert-base-cased-distilled-squad?context=cars&question=What+type+of+animal+is+a+cow%3F), a question answering model based on transformers can be utilized to extract information from the first sentence. \n",
        "\n",
        "The information we obtain is the triple \n",
        "\n",
        "```\n",
        "(semifinal, location, Lusail Stadium)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "R8KurBL1FOEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n",
        "\n",
        "#Since the tokenized article in spacy format is of type <class spacy.tokens.span.Span> and the QA model requires a string, \n",
        "#we need to type convert the sentence to a string by using str()\n",
        "context = str(list(doc.sents)[0])\n",
        "question=\"What was the location of the seminfal?\"\n",
        "\n",
        "#To see which datatype a Python variable is, you can use the type() command\n",
        "print(type(list(doc.sents)[0]))\n",
        "\n",
        "#Here we need to formulate a question that can be answered with the provided context \n",
        "result = question_answerer(question=question, context=context)\n",
        "\n",
        "print(question)\n",
        "print(context)\n",
        "print(\"Answer: \", result['answer'], \"score: \", round(result['score'], 4), \"start: \", result['start'], \"end: \", result['end'])"
      ],
      "metadata": {
        "id": "7zXsQq5qFd0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#TASK #4: \n",
        "\n",
        "Formulate **three** questions to extract information from any of the other sentences in this article. "
      ],
      "metadata": {
        "id": "ixGLz2ZaLoNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your answer for TASK #4: \n",
        "\n",
        "Use the code cell below to provide your answer as code. "
      ],
      "metadata": {
        "id": "sKlGwVjyMJcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Provide the code for your QA TO DO#4 here  "
      ],
      "metadata": {
        "id": "fgpGrIcqL1el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "o_iJCz0KPWcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How easy was this task and did you always obtain the correct answer from the model?"
      ],
      "metadata": {
        "id": "Q8u_OkGML-nM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Use this text cell to provide an answer for the above question regarding your experiments.*\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3Bn6T6RgMTUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "#TASK #5: \n",
        "\n",
        "Formulate a single question and context in another language and run the question-answering model again. No need to search for a question and context online, you can freely make up both on your own. \n",
        "\n",
        "##Your answer for TASK #5:\n"
      ],
      "metadata": {
        "id": "IhGBl6FiO4XW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Provide the code for your QA TO DO#5 in another language here "
      ],
      "metadata": {
        "id": "o5SXWYcIPA0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How well does it work on your own example in another language?\n"
      ],
      "metadata": {
        "id": "CMpOPoZaPO2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Use this text cell to provide an answer for the above question regarding your experiments.*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "PmH0H5koPTnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you are done with all of the above TASKS please upload a link to your notebook on Moodle. "
      ],
      "metadata": {
        "id": "csdHAsRdPZbp"
      }
    }
  ]
}